# Santa Rosa

Fastrak is the California highway toll system.
The cars that drive through the toll have been counted.
We've downloaded the data for hourly counts from each Fastrak stations for
the past 7 years. There are about 8 million rows.
We examine the time series for station 4300, Santa Rosa, because it is
among the most consistent.
to be more consistent than the others.


```{r}
# To produce the report
library(knitr)

# Box cox transformation
library(MASS)

# semitransparent plotting with alpha
library(scales)

# tested helper functions
source('../functions.R')

# contains the `fastrak` data frame with 8 million rows
load('../fastrak.Rda')
```

## Preparation

Significant time was spent preparing this data. 
There were three major issues:

- Missing data
- Too many zeros 
- A month long period where the traffic was double what it should have been
  across all stations

Since we have plenty of data we dropped all of these suspicious points. 
It would be certainly be possible to interpolate or backcast to fill in the
missing data as well.

We dealt with this by writing a small library of tested functions to
prepare the data in a disciplined, repeatable way.

Here is the data of interest.

```{r}
sr1 = getstation(4300, fastrak)

kable(head(sr1))

with(sr1, plot(time, count, col=alpha('black', 0.1)))

fit0 = lm(count ~ poly(time, 3), sr1)
lines(sr1$time, predict(fit0), lwd=4, col='blue')
```

## Trend and Seasonality

We used a linear model in two ways: first to identify more outlying points
where the sensor was only partially active, and second to estimate then
remove the trend and seasonality.

A cubic polynomial was chosen to represent the long term trend because of
we wanted smoothness and given the small magnitude of the trend we felt
that we could do without the complexity of splines. Piecewise linear by
year would have been acceptable as well.

$$
    count = time^3 + month + weekday + hour + weekday:hour + \epsilon
$$

This removes long term trend and seasonality at 3 levels: month, weekday, and
hour.

## Variance stabilizing transform

The box cox method indicated using the cube root function on the counts as
a variance stabilizing transformation.

```{r}
boxcox(lm(count ~ poly(time, 3) + month + weekday*hour, sr1))

# Model formula for subsequent use
f = count^(1/3) ~ poly(time, 3) + month + weekday*hour
```

## Outlier detection and removal

We used a linear model to further identify data points where the sensor may
have stopped working. 

```{r}
fit1 = lm(f, sr1)
```

We do this by removing the lowest 1% of residuals. 
Before removing bottom 1% of residuals we see excessively many large
negative residuals. We suspect this is due to sensor failure.

```{r}
res1 = residuals(fit1)
plot(sr1$time, res1, col=alpha('black', 0.1))
```

For symmetry we used this same threshold on the top of the residuals as well.

```{r}
cutoff = quantile(res1, 0.01)
cutoff
sr2 = sr1[(res1 > cutoff) & (res1 < -cutoff), ]
dim(sr2)
```

This operation cut about 500 from the bottom and 5 from the top.

After trimming outliers:

```{r}
fit2 = lm(f, sr2)
```

```{r}
sr2$res = residuals(fit2)
with(sr2, plot(time, res, col=alpha('black', 0.2)))
```

We still observe the same effect, but it's not as bad. 

## Finding consecutive runs

After this preparation we were ready to pick out the time series.
We did this by finding all runs of consecutively spaced residuals that were
longer than 1000 on the filtered data. There are 5 such groups.

```{r}
diffsr = diff(sr2$time)
islong = longrun(diffsr, 1, 1000)
long = rle(islong)

sr3 = sr2[islong, ]

# TODO - this should be it's own tested function - this code is a bit
# too hacky

# Infer the time spaced groups
a = rle(as.numeric(diff(sr3$time)))
a$values = 1:((length(a$values) + 1) / 2)
a$lengths = 1 + a$lengths[a$lengths != 1]
sr3$group = inverse.rle(a)

# We can add the fitted values in for further analysis
# Exponentiating since we fitted the cube root
sr3$fitted = predict(fit2, sr3) ** 3

dim(sr3)

with(sr3, plot(time, res, col=alpha('black', 0.2)))

save(sr3, file='cleaned.Rda')
```

## Common sense check

We'll first focus on the third
group. For ease of further analysis we'll center and scale the data, and
call the resulting vector $X$.

```{r}
sr33 = sr3[sr3$group == 3, ]
sr33$X = scale(sr33$res)
X = sr33$X

with(sr33, plot(time, X))
```

Let's make sure that the fitting worked using a week's worth of data.

```{r}
small = 600:750
with(sr33[small, ], plot(time, count, lwd=1.5, type='l'))
with(sr33[small, ], lines(time, fitted, lty=2, col='red'))
```

The fitted values look as expected.

Here are the corresponding residuals.

```{r}
with(sr33[small, ], plot(time, X, type='l'))
```

## Time Series

We now can begin the time series analysis. 

```{r}
acf(X)
pacf(X)
```

We'll search over a grid to find the best ARMA model as determined by AIC.

```{r}
# Parameters to search over:
#ap = expand.grid(ar=3:6, ma=3:6)
ap = expand.grid(ar=1:3, ma=1:3)
getaic = function(ar, ma, x=X){
    arima(x, order=c(ar, 0, ma), optim.control=list(maxit=1000))$aic
}
aicvals = mapply(getaic, ap$ar, ap$ma)
plot(aicvals)
```

Note- we needed to extend the maximum number of iterations in order to
compute accurate AIC values for the larger models.
Surprising how fast this becomes computationally expensive.

```{r}
ap[which.min(aicvals), ]

arma1 = arima(X, order=c(6, 0, 6), optim.control=list(maxit=1000))
arma1$aic
```

The AIC criteria chooses an ARMA(6, 6) model.

Check if the residuals resemble white noise.

```{r}
acf(residuals(arma1))
pacf(residuals(arma1))
```

We still see spikes at 24. These make sense because they represent the
following day. Let's compare this with a larger AR model.

```{r}
ar1 = ar(X, method='yw', order.max=35)
ar1$order
```

Yule-Walker is the default.
We'll bump the maximum order from 31 up to 35.

The AR model chose an AR(32) model based on AIC. Let's see how these
residuals look.

```{r}
r1 = na.omit(ar1$resid)
acf(r1)
pacf(r1)
```

They look more like white noise than the ARMA(6, 6) model.
I wonder if the fit looks any different with
other algorithms? The default uses Yule-Walker, but there's also MLE and
OLS to try.

```{r}
ar2 = ar(X, order.max=35, method='ols', intercept=FALSE)
ar2$order
r2 = na.omit(ar2$resid)
acf(r2)
pacf(r2)
```

We don't need to fit an intercept because the data is centered.

So OLS chose an AR(32) model. 

```{r}
ar3 = ar(X, order.max=35, method='burg', intercept=FALSE)
ar3$order
r3 = na.omit(ar3$resid)
acf(r3)
pacf(r3)
```

Surprising that the coefficients are all different. For example, we look at
the first three:

```{r}
first4 = data.frame('yule-walker'=ar1$ar[1:4], 'ols'=ar2$ar[1:4],
           'burg'=ar3$ar[1:4])
first4
```

## Prediction

An hour ahead, a day ahead, or a week ahead.

The adjusted $R^2$ for the ANOVA model is 0.972. We also observe that the
residuals from this model do not exhibit any trend or seasonality.

We conclude that the
ANOVA fit adequately explains the smooth component.

```{r}
s = summary(fit2)
s$adj.r.squared
kable(anova(fit2))
```

To evaluate the performance on the rough part we'll use a fit on the first
1000 data points in the series to predict the remaining 399.

```{r}
ar1000 = ar(X[1:1000], order.max=35)
```

This chooses the much simpler AR(4) model instead of the AR(32) models
chosen previously.

We predict the next 399.

```{r}
preds = predict(ar1000, n.ahead=399)$pred
plot(preds)
```

Lets examine the first 10 predictions compared with the actual values.

```{r}
plot(X[1000:1010], type='l')
lines(preds[1:10])
```

The predictions are very small relative to the size of the residuals.
Conclusion- the ARMA model isn't doing much at all 

Let's compare this with a larger model on data during a different time
period.

```{r}

```
