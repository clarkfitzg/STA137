# Santa Rosa

Fastrak is the California highway toll system.
The cars that drive through the toll have been counted.
We've downloaded the data for hourly counts from each Fastrak stations for
the past 7 years. There are about 8 million rows.
We examine the time series for station 4300, Santa Rosa, because it is
among the most consistent.
to be more consistent than the others.


```{r}
# To produce the report
library(knitr)

# Box cox transformation
library(MASS)

# semitransparent plotting with alpha
library(scales)

# tested helper functions
source('../functions.R')

# contains the `fastrak` data frame with 8 million rows
load('../fastrak.Rda')
```

## Preparation

Significant time was spent preparing this data. 
There were three major issues:

- Missing data
- Too many zeros 
- A month long period where the traffic was double what it should have been
  across all stations

Since we have plenty of data we dropped all of these suspicious points. 
It would be certainly be possible to interpolate or backcast to fill in the
missing data as well.

We dealt with this by writing a small library of tested functions to
prepare the data in a disciplined, repeatable way.

Here is the data of interest.

```{r}
sr1 = getstation(4300, fastrak)

kable(head(sr1))
```

## Trend and Seasonality

We used a general linear model in two ways. First we identified outlying points
where the sensor was only partially active and excluded these questionable
points from further analysis. Second we estimate 
the trend and seasonality.

```{r}
with(sr1, plot(time, count, col=alpha('black', 0.1)))

fit0 = lm(count ~ poly(time, 3), sr1)
lines(sr1$time, predict(fit0), lwd=4, col='blue')
```

This plot indicates that a
cubic polynomial adequately approximates the shape of the long term trend. 
This is preferable to a fit based on the categorical
variable year since the cubic polynomial is smooth and therefore will not
contribute any discontinuities to the time series.

The following general linear model was used:

$$
    count = time + time^2 + time^3 + month + weekday + hour + weekday:hour + \epsilon
$$

Here $time$ is continuous and all other predictor variables are categorical.
$weekday:hour$ is the interaction term between weekday and hour.
Note that the polynomial terms are actually orthogonal polynomials, as is the
default in R's `poly` function. For convenience we express them in this form.

Let $Y_t = count$, $m_t = time + time^2 + time^3$, $s_t = month + weekday +
hour + weekday:hour$, and $X_t = \epsilon$. Then we may express the model
as:

$$
    Y_t = m_t + s_t + X_t
$$

We see that the model can be understood in terms of the classic
linear time series decomposition. The major difference is in the complexity
of the seasonal component. This model accounts for seasonality at three levels: month,
weekday, and hour. Our time series analysis is then on the residual series $(X_t)$.

### Variance stabilizing transform

The box cox method indicated using the cube root function on the counts as
a variance stabilizing transformation.

```{r}
boxcox(lm(count ~ hour + weekday + hour:weekday + month + poly(time, 3), sr1))

# Model formula for subsequent use
f = count^(1/3) ~  hour + weekday + hour:weekday + month + poly(time, 3)
```

## Outlier detection and removal

The initial fit of the linear model identified suspicious data points where the sensor may
have stopped working.

```{r}
fit1 = lm(f, sr1)
res1 = residuals(fit1)
plot(sr1$time, res1, col=alpha('black', 0.1))
```

Before removing the bottom 1% of residuals we see excessively many large
negative residuals.
For symmetry we used this same threshold on the top of the residuals as well.

```{r}
cutoff = quantile(res1, 0.01)
cutoff
sr2 = sr1[(res1 > cutoff) & (res1 < -cutoff), ]
dim(sr2)
```

This operation cut about 500 points from the bottom and 5 from the top.

These are the residuals after trimming outliers:

```{r}
fit2 = lm(f, sr2)
```

```{r}
sr2$res = residuals(fit2)
with(sr2, plot(time, res, col=alpha('black', 0.2)))
```

We still observe the same effect, but it's not as bad. 

### Interpretation

Experience tells us that traffic varies primarily according to time of 
day, and also by the day of the week. 

This analysis confirms this.
#TODO- 

$$
    count^{(1/3)} = time + time^2 + time^3 + month + weekday + hour + weekday:hour + \epsilon
$$


```{r}
a2 = anova(fit2)
a2$percent = a2[, 'Sum Sq'] / sum(a2[, 'Sum Sq'])
kable(a2)
```



## Finding consecutive runs

After this preparation we were ready to pick out the time series.
We did this by finding all runs of consecutively spaced residuals that were
longer than 1000 on the filtered data. There are 5 such groups.

```{r}
diffsr = diff(sr2$time)
islong = longrun(diffsr, 1, 1000)
long = rle(islong)

sr3 = sr2[islong, ]

# TODO - this should be it's own tested function - this code is a bit
# too hacky

# Infer the time spaced groups
a = rle(as.numeric(diff(sr3$time)))
a$values = 1:((length(a$values) + 1) / 2)
a$lengths = 1 + a$lengths[a$lengths != 1]
sr3$group = inverse.rle(a)

# We can add the fitted values in for further analysis
# Exponentiating since we fitted the cube root
sr3$fitted = predict(fit2, sr3) ** 3

dim(sr3)

with(sr3, plot(time, res, col=alpha('black', 0.2)))

save(sr3, file='cleaned.Rda')
```

## Common sense check

We'll first focus on the third
group. For ease of further analysis we'll center and scale the data, and
call the resulting vector $X$.

```{r}
sr33 = sr3[sr3$group == 3, ]
sr33$X = scale(sr33$res)
X = sr33$X

with(sr33, plot(time, X))
```

Let's make sure that the fitting worked using a week's worth of data.

```{r}
small = 600:750
with(sr33[small, ], plot(time, count, lwd=1.5, type='l'))
with(sr33[small, ], lines(time, fitted, lty=2, col='red'))
```

The fitted values look as expected.

Here are the corresponding residuals.

```{r}
with(sr33[small, ], plot(time, X, type='l'))
```

## Time Series

We now can begin the time series analysis. 

```{r}
acf(X)
pacf(X)
```

We'll search over a grid to find the best ARMA model as determined by AIC.

```{r}
# Parameters to search over:
#ap = expand.grid(ar=3:6, ma=3:6)
ap = expand.grid(ar=1:3, ma=1:3)
getaic = function(ar, ma, x=X){
    arima(x, order=c(ar, 0, ma), optim.control=list(maxit=1000))$aic
}
aicvals = mapply(getaic, ap$ar, ap$ma)
plot(aicvals)
```

Note- we needed to extend the maximum number of iterations in order to
compute accurate AIC values for the larger models.
Surprising how fast this becomes computationally expensive.

```{r}
ap[which.min(aicvals), ]

arma1 = arima(X, order=c(6, 0, 6), optim.control=list(maxit=1000))
arma1$aic
```

The AIC criteria chooses an ARMA(6, 6) model.

Check if the residuals resemble white noise.

```{r}
acf(residuals(arma1))
pacf(residuals(arma1))
```

We still see spikes at 24. These make sense because they represent the
following day. Let's compare this with a larger AR model.

```{r}
ar1 = ar(X, method='yw', order.max=35)
ar1$order
```

Yule-Walker is the default.
We'll bump the maximum order from 31 up to 35.

The AR model chose an AR(32) model based on AIC. Let's see how these
residuals look.

```{r}
r1 = na.omit(ar1$resid)
acf(r1)
pacf(r1)
```

They look more like white noise than the ARMA(6, 6) model.
I wonder if the fit looks any different with
other algorithms? The default uses Yule-Walker, but there's also MLE and
OLS to try.

```{r}
ar2 = ar(X, order.max=35, method='ols', intercept=FALSE)
ar2$order
r2 = na.omit(ar2$resid)
acf(r2)
pacf(r2)
```

We don't need to fit an intercept because the data is centered.

So OLS chose an AR(32) model. 

```{r}
ar3 = ar(X, order.max=35, method='burg', intercept=FALSE)
ar3$order
r3 = na.omit(ar3$resid)
acf(r3)
pacf(r3)
```

Surprising that the coefficients are all different. For example, we look at
the first three:

```{r}
first4 = data.frame('yule-walker'=ar1$ar[1:4], 'ols'=ar2$ar[1:4],
           'burg'=ar3$ar[1:4])
first4
```

## Prediction

An hour ahead, a day ahead, or a week ahead.

The adjusted $R^2$ for the ANOVA model is 0.972. We also observe that the
residuals from this model do not exhibit any trend or seasonality.

We conclude that the
ANOVA fit adequately explains the smooth component.

```{r}
s = summary(fit2)
s$adj.r.squared
kable(anova(fit2))
```

To evaluate the performance on the rough part we'll use a fit on the first
1300 data points in the series to predict the remaining 99.

```{r}
ar1300 = ar(X[1:1300], order.max=35)
```

This chooses the much simpler AR(4) model instead of the AR(32) models
chosen previously.

We predict the next 99.

```{r}
preds = predict(ar1300, n.ahead=99)$pred
plot(preds)
```

Lets examine the first 10 predictions compared with the actual values.

```{r}
d = sr33$time[1301:1399]
plot(d, X[1301:1399], type='l')
lines(d, preds, col='blue')
```

# Computing

The initial size of this data set, 8 million points, presented some unique
computational challenges. 

We used R's biglm package together with 
chunking to fit a linear ANOVA model with 5000 rows. It took 5 days to run on the
department server (gumbel). Additionally, it used all 256 GB of memory 
available.
It's possible that different chunk sizes would have performed
better. `lme4` solves the problem with sparse matrices, which is 
ideal, but it wouldn't fit the full model, even with the 256 GB of
memory on the server.
