# Santa Rosa

Fastrak is the California highway toll system.
The cars that drive through the toll have been counted.
We've downloaded the data for hourly counts from each Fastrak stations for
the past 7 years. There are about 8 million rows.
We examine the time series for station 4300, Santa Rosa, because it is
among the most consistent.
to be more consistent than the others.


```{r}
# To produce the report
library(knitr)

# Box cox transformation
library(MASS)

# semitransparent plotting with alpha
library(scales)

# tested helper functions
source('../functions.R')

# contains the `fastrak` data frame with 8 million rows
load('../fastrak.Rda')
```

## Preparation

Significant time was spent preparing this data. 
There were three major issues:

- Missing data
- Too many zeros 
- A month long period where the traffic was double what it should have been
  across all stations

Since we have plenty of data we dropped all of these suspicious points. 
It would be certainly be possible to interpolate or backcast to fill in the
missing data as well.

We dealt with this by writing a small library of tested functions to
prepare the data in a disciplined, repeatable way.

Here is the data of interest.

```{r}
sr1 = getstation(4300, fastrak)

kable(head(sr1))

with(sr1, plot(time, count, col=alpha('black', 0.1)))

fit0 = lm(count ~ poly(time, 3), sr1)
lines(sr1$time, predict(fit0), lwd=4, col='blue')
```

## Trend and Seasonality

We used a linear model in two ways: first to identify more outlying points
where the sensor was only partially active, and second to estimate then
remove the trend and seasonality.

A cubic polynomial was chosen to represent the long term trend because of
we wanted smoothness and given the small magnitude of the trend we felt
that we could do without the complexity of splines. Piecewise linear by
year would have been acceptable as well.

$$
    count = time^3 + month + weekday + hour + weekday:hour + \epsilon
$$

This removes long term trend and seasonality at 3 levels: month, weekday, and
hour.

## Variance stabilizing transform

The box cox method indicated using the cube root function on the counts as
a variance stabilizing transformation.

```{r}
boxcox(lm(count ~ poly(time, 3) + month + weekday*hour, sr1))

# Model formula for subsequent use
f = count^(1/3) ~ poly(time, 3) + month + weekday*hour
```

## Outlier detection and removal

We used a linear model to further identify data points where the sensor may
have stopped working. 

```{r}
fit1 = lm(f, sr1)
```

We do this by removing the lowest 1% of residuals. 
Before removing bottom 1% of residuals we see excessively many large
negative residuals. We suspect this is due to sensor failure.

```{r}
res1 = residuals(fit1)
plot(sr1$time, res1, col=alpha('black', 0.1))
```

For symmetry we used this same threshold on the top of the residuals as well.

```{r}
cutoff = quantile(res1, 0.01)
cutoff
sr2 = sr1[(res1 > cutoff) & (res1 < -cutoff), ]
dim(sr2)
```

This operation cut about 500 from the bottom and 5 from the top.

After trimming outliers:

```{r}
fit2 = lm(f, sr2)
```

```{r}
sr2$res = residuals(fit2)
with(sr2, plot(time, res, col=alpha('black', 0.2)))
```

We still observe the same effect, but it's not as bad. 

## Finding consecutive runs

After this preparation we were ready to pick out the time series.
We did this by finding all runs of consecutively spaced residuals that were
longer than 1000 on the filtered data. There are 5 such groups.

```{r}
diffsr = diff(sr2$time)
islong = longrun(diffsr, 1, 1000)
long = rle(islong)

sr3 = sr2[islong, ]

# TODO - this should be it's own tested function - this code is a bit
# too hacky

# Infer the time spaced groups
a = rle(as.numeric(diff(sr3$time)))
a$values = 1:((length(a$values) + 1) / 2)
a$lengths = 1 + a$lengths[a$lengths != 1]
sr3$group = inverse.rle(a)

# We can add the fitted values in for further analysis
# Exponentiating since we fitted the cube root
sr3$fitted = predict(fit2, sr3) ** 3

dim(sr3)

with(sr3, plot(time, res, col=alpha('black', 0.2)))
```

## Common sense check

We'll first focus on the third
group. For ease of further analysis we'll center and scale the data, and
call the resulting vector $X$.

```{r}
sr33 = sr3[sr3$group == 3, ]
sr33$X = scale(sr33$res)
X = sr33$X

with(sr33, plot(time, X))
```

Let's make sure that the fitting worked using a week's worth of data.

```{r}
small = 600:750
with(sr33[small, ], plot(time, count, lwd=1.5, type='l'))
with(sr33[small, ], lines(time, fitted, lty=2, col='red'))
```

The fitted values look as expected.

Here are the corresponding residuals.

```{r}
with(sr33[small, ], plot(time, X, type='l'))
```

## Time Series

We now can begin the time series analysis. 

```{r}
acf(X)
pacf(X)
```

